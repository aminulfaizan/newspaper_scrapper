{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGz7jftNYqwrI/PX5bHpAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminulfaizan/newspaper_scrapper/blob/newsbranch/newspaper_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## you can create your spider by the below's command line"
      ],
      "metadata": {
        "id": "cEK82iqGvbmk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpse2JWQAi_A",
        "outputId": "e5e9aa90-26b8-40e7-97a8-9b846bc1f963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "id": "HjXbAAEwAoK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy startproject news_scrapy"
      ],
      "metadata": {
        "id": "cPNvyy1LA5zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd news_scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hzn0XYsBHKd",
        "outputId": "fec11bf4-fbca-45cb-dbe2-6d3a088f3064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/news_scrapy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy genspider spider_name domainname.com"
      ],
      "metadata": {
        "id": "uWlQ73_IBRy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## your spider should looks like"
      ],
      "metadata": {
        "id": "3uI_SDpovnO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from scrapy import Request\n",
        "\n",
        "class SpiderNameSpider(scrapy.Spider):\n",
        "    name = \"spider_name\"\n",
        "    allowed_domains = [\"domainname.com\"]\n",
        "    start_urls = [\"http://domainname.com/\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "    def start_requests(self):\n",
        "        # Set the date range and number of pages to scrape\n",
        "        date_from = 'startdate'\n",
        "        date_to = 'enddate'\n",
        "        num_pages = 3\n",
        "\n",
        "        # Generate the URL for each page to scrape\n",
        "        for page_num in range(1, num_pages + 1):\n",
        "            url = f'http://domainname.com/archive?dateFrom={date_from}&dateTo={date_to}&page={page_num}'\n",
        "            yield Request(url=url, callback=self.parse)\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Extract the news article links from the current page\n",
        "        article_links = response.xpath('//div[@class=\"col-md-9 col-sm-8 col-xs-12\"]/div[@class=\"row\"]//h4/a/@href')\n",
        "\n",
        "        # Follow each link and extract the article details\n",
        "        for link in article_links:\n",
        "            yield Request(url=link.extract(), callback=self.parse_article)\n",
        "\n",
        "    def parse_article(self, response):\n",
        "        # Extract the article details using Xpath selectors\n",
        "        title = response.xpath('//h1/text()').get().strip()\n",
        "        author = response.xpath('//span[@class=\"name\"]/text()').get()\n",
        "        date = response.xpath('//span[@class=\"date\"]/text()').get()\n",
        "        content = response.xpath('//div[@class=\"news-content-body\"]/p/text() | //div[@class=\"news-content-body\"]/p/strong/text() | //div[@class=\"news-content-body\"]/p/em/text()')\n",
        "        content = ' '.join(content.getall()).strip()\n",
        "\n",
        "        # Create a dictionary containing the article details and yield it\n",
        "        #you may add or change the given dictionary according to your desire\n",
        "        yield {\n",
        "            'title': title,\n",
        "            'author': author,\n",
        "            'date': date,\n",
        "            'content': content\n",
        "        }\n"
      ],
      "metadata": {
        "id": "KiSl4B7Jvth8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}